{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uIneoe7XpLqF"
   },
   "source": [
    "### 9.1.7 최적화 알고리즘 구현 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNgM0Ve_pEXW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import cupy as np  # GPU를 사용하면 주석 해제\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -- 각 설정값 --\n",
    "img_size = 8  # 이미지의 높이와 폭\n",
    "n_mid = 64  # 은닉층 뉴런 수\n",
    "n_out = 10\n",
    "eta = 0.001  # 학습률\n",
    "epochs = 21 \n",
    "batch_size = 32\n",
    "interval = 5  # 경과표시 간격\n",
    "\n",
    "digits_data = datasets.load_digits()\n",
    "\n",
    "# -- Momentum --\n",
    "class MomentumOptimizer:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        self.dif_params = None\n",
    "\n",
    "    def update(self, eta, params, grads):\n",
    "        if self.dif_params is None:\n",
    "            self.dif_params = [np.zeros_like(param) for param in params]  # 컴프리헨션\n",
    "        for param, grad, dif_param in zip(params, grads, self.dif_params):\n",
    "            dif_param[:] = -eta*grad + self.alpha*dif_param\n",
    "            param += dif_param           \n",
    "\n",
    "# -- 입력 데이터 --\n",
    "input_data = np.asarray(digits_data.data)\n",
    "input_data = (input_data - np.average(input_data)) / np.std(input_data)  # 평균 0, 표준편차 1\n",
    "\n",
    "# -- 정답 데이터 --\n",
    "correct = np.asarray(digits_data.target)\n",
    "correct_data = np.zeros((len(correct), n_out))\n",
    "for i in range(len(correct)):\n",
    "    correct_data[i, correct[i]] = 1  # 원핫 인코딩\n",
    "\n",
    "# -- 훈련 데이터와 테스트 데이터 분할 --\n",
    "x_train, x_test, t_train, t_test = train_test_split(input_data, correct_data)\n",
    "\n",
    "# -- 전결합층의 부모 클래스 --\n",
    "class BaseLayer:\n",
    "    def __init__(self):\n",
    "        self.optimizer = MomentumOptimizer(0.9)\n",
    "\n",
    "    def update(self, eta):\n",
    "        self.optimizer.update(eta, [self.w, self.b], [self.grad_w, self.grad_b])\n",
    "\n",
    "# -- 은닉층 --\n",
    "class MiddleLayer(BaseLayer):\n",
    "    def __init__(self, n_upper, n):\n",
    "        super().__init__()\n",
    "        self.w = np.random.randn(n_upper, n) * np.sqrt(2/n_upper)  # He 초깃값\n",
    "        self.b = np.zeros(n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.u = np.dot(x, self.w) + self.b\n",
    "        self.y = np.where(self.u <= 0, 0, self.u) # ReLU\n",
    "    \n",
    "    def backward(self, grad_y):\n",
    "        delta = grad_y * np.where(self.u <= 0, 0, 1)  # ReLU 미분\n",
    "\n",
    "        self.grad_w = np.dot(self.x.T, delta)\n",
    "        self.grad_b = np.sum(delta, axis=0)\n",
    "        self.grad_x = np.dot(delta, self.w.T) \n",
    "\n",
    "# -- 출력층 --\n",
    "class OutputLayer(BaseLayer):\n",
    "    def __init__(self, n_upper, n):\n",
    "        super().__init__()\n",
    "        self.w = np.random.randn(n_upper, n) / np.sqrt(n_upper)  # # 자비에르 초기화 기반의 초깃값\n",
    "        self.b = np.zeros(n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        u = np.dot(x, self.w) + self.b\n",
    "        self.y = np.exp(u)/np.sum(np.exp(u), axis=1, keepdims=True)  # 소프트맥스 함수\n",
    "\n",
    "    def backward(self, t):\n",
    "        delta = self.y - t\n",
    "        \n",
    "        self.grad_w = np.dot(self.x.T, delta)\n",
    "        self.grad_b = np.sum(delta, axis=0)\n",
    "        self.grad_x = np.dot(delta, self.w.T)\n",
    "\n",
    "# -- 각 층의 초기화 --\n",
    "layers = [MiddleLayer(img_size*img_size, n_mid),\n",
    "          MiddleLayer(n_mid, n_mid),\n",
    "          OutputLayer(n_mid, n_out)]\n",
    "\n",
    "# -- 순전파 --\n",
    "def forward_propagation(x):\n",
    "    for layer in layers:\n",
    "        layer.forward(x)\n",
    "        x = layer.y\n",
    "    return x\n",
    "\n",
    "# -- 역전파 --\n",
    "def backpropagation(t):\n",
    "    grad_y = t\n",
    "    for layer in reversed(layers):\n",
    "        layer.backward(grad_y)\n",
    "        grad_y = layer.grad_x\n",
    "    return grad_y\n",
    "\n",
    "# -- 파라미터 갱신 --\n",
    "def update_params():\n",
    "    for layer in layers:\n",
    "        layer.update(eta)\n",
    "\n",
    "# -- 오차 계산 --\n",
    "def get_error(x, t):\n",
    "    y = forward_propagation(x)\n",
    "    return -np.sum(t*np.log(y+1e-7)) / len(y)  # 교차 엔트로피 오차\n",
    "\n",
    "# -- 정확도 측정 --\n",
    "def get_accuracy(x, t):\n",
    "    y = forward_propagation(x)\n",
    "    count = np.sum(np.argmax(y, axis=1) == np.argmax(t, axis=1))\n",
    "    return count / len(y)\n",
    "\n",
    "# -- 오차 기록 --\n",
    "error_record_train = []\n",
    "error_record_test = []\n",
    "\n",
    "n_batch = len(x_train) // batch_size  # 1에포크당 배치 수\n",
    "for i in range(epochs):\n",
    "\n",
    "    # -- 학습 -- \n",
    "    index_random = np.arange(len(x_train))\n",
    "    np.random.shuffle(index_random)  # 인덱스 임의섞기\n",
    "    for j in range(n_batch):\n",
    "        \n",
    "        # 미니 배치\n",
    "        mb_index = index_random[j*batch_size : (j+1)*batch_size]\n",
    "        x_mb = x_train[mb_index, :]\n",
    "        t_mb = t_train[mb_index, :]\n",
    "        \n",
    "        # 순전파와 역전파\n",
    "        forward_propagation(x_mb)\n",
    "        backpropagation(t_mb)\n",
    "        \n",
    "        # 파라미터 갱신\n",
    "        update_params()\n",
    "\n",
    "    # -- 오차 계산과 기록 --  \n",
    "    error_train = get_error(x_train, t_train)\n",
    "    error_record_train.append(error_train)\n",
    "    error_test = get_error(x_test, t_test)\n",
    "    error_record_test.append(error_test)\n",
    "    \n",
    "    # -- 경과 표시 -- \n",
    "    if i%interval == 0:\n",
    "        print(\"Epoch:\" + str(i+1) + \"/\" + str(epochs),\n",
    "              \"Error_train: \" + str(error_train),\n",
    "              \"Error_test: \" + str(error_test))\n",
    "        \n",
    "# -- 오차 추이 그래프 표시 -- \n",
    "plt.plot(range(1, len(error_record_train)+1), error_record_train, label=\"Train\")\n",
    "plt.plot(range(1, len(error_record_test)+1), error_record_test, label=\"Test\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()\n",
    "\n",
    "# -- 정확도 계산 -- \n",
    "acc_train = get_accuracy(x_train, t_train)\n",
    "acc_test = get_accuracy(x_test, t_test)\n",
    "\n",
    "print(\"Acc_train: \"+str(acc_train*100)+\"%\", \"Acc_test: \"+str(acc_test*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3D75dj_Zph2t"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import cupy as np  # GPU를 사용하면 주석 해제\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -- 각 설정값 --\n",
    "img_size = 8  # 이미지 높아와 폭\n",
    "n_mid = 16  # 은닉층 뉴런 수\n",
    "n_out = 10\n",
    "eta = 0.01  # 학습률\n",
    "epochs = 51 \n",
    "batch_size = 32\n",
    "interval = 5  # 경과 표시 간격\n",
    "\n",
    "digits_data = datasets.load_digits()\n",
    "\n",
    "# -- AdaGrad --\n",
    "class AdaGradOptimizer:\n",
    "    def __init__(self):\n",
    "        self.hs = None\n",
    "\n",
    "    def update(self, eta, params, grads):\n",
    "        if self.hs is None:\n",
    "            self.hs = [np.zeros_like(param)+1e-7 for param in params]\n",
    "        for param, grad, h in zip(params, grads, self.hs):\n",
    "            h += grad**2\n",
    "            param -= eta / np.sqrt(h) * grad      \n",
    "\n",
    "# -- 입력 데이터 --\n",
    "input_data = np.asarray(digits_data.data)\n",
    "input_data = (input_data - np.average(input_data)) / np.std(input_data)  # 평균 0, 표준편차 1\n",
    "\n",
    "# -- 정답 덷이터 --\n",
    "correct = np.asarray(digits_data.target)\n",
    "correct_data = np.zeros((len(correct), n_out))\n",
    "for i in range(len(correct)):\n",
    "    correct_data[i, correct[i]] = 1  # 원핫인코딩\n",
    "\n",
    "# -- 훈련 데이터와 테스트 데이터 분할--\n",
    "x_train, x_test, t_train, t_test = train_test_split(input_data, correct_data)\n",
    "\n",
    "# -- 전결합층의 부모 클래스 --\n",
    "class BaseLayer:\n",
    "    def __init__(self):\n",
    "        self.optimizer = AdaGradOptimizer()\n",
    "\n",
    "    def update(self, eta):\n",
    "        self.optimizer.update(eta, [self.w, self.b], [self.grad_w, self.grad_b])\n",
    "\n",
    "# -- 은닉층 --\n",
    "class MiddleLayer(BaseLayer):\n",
    "    def __init__(self, n_upper, n):\n",
    "        super().__init__()\n",
    "        self.w = np.random.randn(n_upper, n) * np.sqrt(2/n_upper)  # He 초깃값\n",
    "        self.b = np.zeros(n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.u = np.dot(x, self.w) + self.b\n",
    "        self.y = np.where(self.u <= 0, 0, self.u) # ReLU\n",
    "    \n",
    "    def backward(self, grad_y):\n",
    "        delta = grad_y * np.where(self.u <= 0, 0, 1)  # ReLU 미분\n",
    "\n",
    "        self.grad_w = np.dot(self.x.T, delta)\n",
    "        self.grad_b = np.sum(delta, axis=0)\n",
    "        self.grad_x = np.dot(delta, self.w.T) \n",
    "\n",
    "# -- 출력층 --\n",
    "class OutputLayer(BaseLayer):\n",
    "    def __init__(self, n_upper, n):\n",
    "        super().__init__()\n",
    "        self.w = np.random.randn(n_upper, n) / np.sqrt(n_upper)  # 자비에르 초기화 기반의 초깃값\n",
    "        self.b = np.zeros(n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        u = np.dot(x, self.w) + self.b\n",
    "        self.y = np.exp(u)/np.sum(np.exp(u), axis=1, keepdims=True)  # 소프트맥스 함수\n",
    "\n",
    "    def backward(self, t):\n",
    "        delta = self.y - t\n",
    "        \n",
    "        self.grad_w = np.dot(self.x.T, delta)\n",
    "        self.grad_b = np.sum(delta, axis=0)\n",
    "        self.grad_x = np.dot(delta, self.w.T)\n",
    "\n",
    "# -- 각 층의 초기화 --\n",
    "layers = [MiddleLayer(img_size*img_size, n_mid),\n",
    "          MiddleLayer(n_mid, n_mid),\n",
    "          OutputLayer(n_mid, n_out)]\n",
    "\n",
    "# -- 순전파 --\n",
    "def forward_propagation(x):\n",
    "    for layer in layers:\n",
    "        layer.forward(x)\n",
    "        x = layer.y\n",
    "    return x\n",
    "\n",
    "# -- 역전파 --\n",
    "def backpropagation(t):\n",
    "    grad_y = t\n",
    "    for layer in reversed(layers):\n",
    "        layer.backward(grad_y)\n",
    "        grad_y = layer.grad_x\n",
    "    return grad_y\n",
    "\n",
    "# -- 파라미터 갱신 --\n",
    "def update_params():\n",
    "    for layer in layers:\n",
    "        layer.update(eta)\n",
    "\n",
    "# -- 오차 계산 --\n",
    "def get_error(x, t):\n",
    "    y = forward_propagation(x)\n",
    "    return -np.sum(t*np.log(y+1e-7)) / len(y)  # 교차엔트로피 오차\n",
    "\n",
    "# -- 정확도 계산 --\n",
    "def get_accuracy(x, t):\n",
    "    y = forward_propagation(x)\n",
    "    count = np.sum(np.argmax(y, axis=1) == np.argmax(t, axis=1))\n",
    "    return count / len(y)\n",
    "\n",
    "# -- 오차 기록 --\n",
    "error_record_train = []\n",
    "error_record_test = []\n",
    "\n",
    "n_batch = len(x_train) // batch_size  # 1에포크당 배치 개수\n",
    "for i in range(epochs):\n",
    "\n",
    "    # -- 학습 -- \n",
    "    index_random = np.arange(len(x_train))\n",
    "    np.random.shuffle(index_random)  # 인덱스 임의섞기\n",
    "    for j in range(n_batch):\n",
    "        \n",
    "        #  미니 배치\n",
    "        mb_index = index_random[j*batch_size : (j+1)*batch_size]\n",
    "        x_mb = x_train[mb_index, :]\n",
    "        t_mb = t_train[mb_index, :]\n",
    "        \n",
    "        # 순전파와 역전파\n",
    "        forward_propagation(x_mb)\n",
    "        backpropagation(t_mb)\n",
    "        \n",
    "        # 파라미터 갱신\n",
    "        update_params()\n",
    "\n",
    "    # -- 오차 계산 및 기록 --  \n",
    "    error_train = get_error(x_train, t_train)\n",
    "    error_record_train.append(error_train)\n",
    "    error_test = get_error(x_test, t_test)\n",
    "    error_record_test.append(error_test)\n",
    "    \n",
    "    # -- 경과 표시 -- \n",
    "    if i%interval == 0:\n",
    "        print(\"Epoch:\" + str(i+1) + \"/\" + str(epochs),\n",
    "              \"Error_train: \" + str(error_train),\n",
    "              \"Error_test: \" + str(error_test))\n",
    "        \n",
    "# -- 오차 추이 표시 그래프 -- \n",
    "plt.plot(range(1, len(error_record_train)+1), error_record_train, label=\"Train\")\n",
    "plt.plot(range(1, len(error_record_test)+1), error_record_test, label=\"Test\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()\n",
    "\n",
    "# -- 정확도 계산 -- \n",
    "acc_train = get_accuracy(x_train, t_train)\n",
    "acc_test = get_accuracy(x_test, t_test)\n",
    "\n",
    "print(\"Acc_train: \"+str(acc_train*100)+\"%\", \"Acc_test: \"+str(acc_test*100)+\"%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMwFP6uTWr8dNMBS6zFvbrD",
   "collapsed_sections": [],
   "name": "9-1.optimizer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
